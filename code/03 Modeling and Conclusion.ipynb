{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: The Persuasive Power of Words\n",
    "\n",
    "*by Nee Bimin*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3: Modeling and Conclusion\n",
    "\n",
    "In this notebook, we will predict the number of ratings per view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "- [Pre-processing](#Preprocessing)\n",
    "    * [Tokenizing and Lemmatizing](#Tokenizing-and-Lemmatizing)\n",
    "- [Train/Test Split](#Train/Test-Split)\n",
    "- [Grid Search CV](#Grid-Search-CV)\n",
    "    * [Baseline Accuracy](#Baseline-Accuracy)\n",
    "    * [Count Vectorizer](#Count-Vectorizer)\n",
    "    * [Tfidf Vectorizer](#Tfidf-Vectorizer)\n",
    "- [Optimising Tfidf Multinomial Naive Bayes](#Optimising-Tfidf-Multinomial-Naive_Bayes)\n",
    "- [Optimising Tfidf Logistic Regression](#Optimising-Tfidf-Logistic-Regression)\n",
    "- [Conclusion-and-Recommendations](#Conclusion-and-Recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yvettenee/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning:\n",
      "\n",
      "calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import graphviz\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# modeling imports\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "ted_model = pd.read_csv('../data/ted_model.csv')\n",
    "transcripts = pd.read_csv('../data/transcripts_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = []\n",
    "\n",
    "for i in range(len(transcripts['transcript'])):\n",
    "    loop_tokens = tokenizer.tokenize(transcripts['transcript'].iloc[i].lower())\n",
    "    \n",
    "    for j, token in enumerate(loop_tokens):\n",
    "            # Remove non-letters\n",
    "            re.sub('[^a-zA-Z]', '', token)\n",
    "\n",
    "            # Remove stopwords   \n",
    "            if token in stopwords.words('english'):\n",
    "                loop_tokens[j] = ''\n",
    "                \n",
    "    tokens.append(loop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'voice', 'mail', '', 'old', 'friend', '', '', 'called', '']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check token\n",
    "tokens[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'voice', 'mail', '', 'old', 'friend', '', '', 'called', '']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "speech_token_lem = []\n",
    "for speech in tokens:\n",
    "    speech_lem = []\n",
    "    \n",
    "    for word in speech:\n",
    "        #print(word)\n",
    "        word_lem = lem.lemmatize(word) # get lemmatized word\n",
    "        speech_lem.append(word_lem) # add to post list\n",
    "    speech_token_lem.append(speech_lem)  # add post list to lemma matrix\n",
    "\n",
    "# Check lemmatized token\n",
    "speech_token_lem[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format tokenized lemma for vectorizer i.e. change to a list of strings\n",
    "lem_list = []\n",
    "\n",
    "for speech in speech_token_lem:\n",
    "    lem_list.append(' '.join(speech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasive_model = pd.DataFrame(lem_list, columns=['transcript'])\n",
    "persuasive_model['persuasive'] = ted_model['persuasive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     2467.000000\n",
       "mean       226.397649\n",
       "std        473.497627\n",
       "min          0.000000\n",
       "25%         39.000000\n",
       "50%        101.000000\n",
       "75%        234.000000\n",
       "max      10704.000000\n",
       "Name: persuasive, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasive_model['persuasive'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another column to label a talk as persuasive if the number of persuasive votes\n",
    "# is greater than or equal to the median\n",
    "persuasive_median = persuasive_model['persuasive'].median()\n",
    "persuasive_model['persuasive_label'] = np.where(persuasive_model['persuasive'] >= persuasive_median, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inspiring_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-ab1021d9ed77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create another column to label a talk as inspiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# if the number of inspiring votes is greater than or equal to the median\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minspiring_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspiring_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inspiring'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0minspiring_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inspiring_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspiring_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inspiring'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0minspiring_median\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inspiring_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Create another column to label a talk as inspiring \n",
    "# if the number of inspiring votes is greater than or equal to the median\n",
    "inspiring_median = inspiring_model['inspiring'].median()\n",
    "inspiring_model['inspiring_label'] = np.where(inspiring_model['inspiring'] >= inspiring_median, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = persuasive_model['transcript']\n",
    "y = persuasive_model['persuasive_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it is classification problem, we will stratify y to ensure equal split of X and y \n",
    "# in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in X_train: 1850\n",
      "Number of rows in y_train: 1850\n",
      "Number of rows in X_test: 617\n",
      "Number of rows in y_test: 617\n"
     ]
    }
   ],
   "source": [
    "# Confirm that train and test variables have the same length\n",
    "print('Number of rows in X_train: {}'.format(len(X_train)))\n",
    "print('Number of rows in y_train: {}'.format(len(y_train)))\n",
    "print('Number of rows in X_test: {}'.format(len(X_test)))\n",
    "print('Number of rows in y_test: {}'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5064864864864865"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "baseline = y_train.value_counts(normalize=True)[1]\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find the baseline accuracy, which is the likelihood of a transcript being persuasive, by calculating the percentage of the dataset that has the target value of 1. Normalising the value counts shows the percentage, and gives a baseline accuracy of 50.6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline steps for each combination of model\n",
    "# Include standard scaler for knn and logistic regression because distance are important when classifying\n",
    "steps_list_cv = [ \n",
    "    [('cv', CountVectorizer()),('multi_nb', MultinomialNB())],\n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('knn', KNeighborsClassifier())], \n",
    "    [('cv', CountVectorizer()),('scaler', StandardScaler(with_mean=False)),('logreg', LogisticRegression())]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_titles_cv = ['multi_nb + cv','knn + cv','logreg + cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_cv = [\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)]},\n",
    "    {'cv__stop_words':['english'], 'cv__ngram_range':[(1,1),(1,2)]}\n",
    "]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, best_params, train_accuracy, test_accuracy, baseline_accuracy, recall, precision, f1-score]\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate results DataFrame for CountVectorizer\n",
    "gs_results_cv = pd.DataFrame(columns=['model','best_params','train_accuracy','test_accuracy',\n",
    "                                      'baseline_accuracy','recall', 'precision', 'f1-score'])\n",
    "gs_results_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  multi_nb + cv\n",
      "Best Params:  {'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n",
      "0.9983783783783784 \n",
      "\n",
      "0.49108589951377635 \n",
      "\n",
      "True Negatives: 124\n",
      "False Positives: 196\n",
      "False Negatives: 118\n",
      "True Positives: 179 \n",
      "\n",
      "Model:  knn + cv\n",
      "Best Params:  {'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n",
      "0.5064864864864865 \n",
      "\n",
      "0.4813614262560778 \n",
      "\n",
      "True Negatives: 0\n",
      "False Positives: 320\n",
      "False Negatives: 0\n",
      "True Positives: 297 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yvettenee/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/yvettenee/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  logreg + cv\n",
      "Best Params:  {'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n",
      "0.9989189189189189 \n",
      "\n",
      "0.5024311183144247 \n",
      "\n",
      "True Negatives: 135\n",
      "False Positives: 185\n",
      "False Negatives: 122\n",
      "True Positives: 175 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through index of number of steps\n",
    "for i in range(len(steps_list_cv)):\n",
    "    # instantiate pipeline \n",
    "    pipe = Pipeline(steps=steps_list_cv[i])\n",
    "    # fit GridSearchCV to model and model's params\n",
    "    gs = GridSearchCV(pipe, pipe_params_cv[i], cv=3) \n",
    "\n",
    "    model_results = {}\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    print('Model: ', steps_titles_cv[i])\n",
    "    model_results['model'] = steps_titles_cv[i]\n",
    "\n",
    "    print('Best Params: ', gs.best_params_)\n",
    "    model_results['best_params'] = gs.best_params_\n",
    "\n",
    "    print('Cross val accuracy: ', gs.score(X_train, y_train), '\\n')\n",
    "    model_results['train_accuracy'] = gs.score(X_train, y_train)\n",
    "    \n",
    "    print('Test accuracy: ', gs.score(X_test, y_test), '\\n')\n",
    "    model_results['test_accuracy'] = gs.score(X_test, y_test)\n",
    "    \n",
    "    model_results['baseline_accuracy'] = baseline\n",
    "    \n",
    "    # Display the confusion matrix results showing true/false positive/negative\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, gs.predict(X_test)).ravel() \n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp, '\\n')\n",
    "    \n",
    "    model_results['recall'] = tp/(tp+fn)\n",
    "    model_results['precision'] = tp/(tp+fp)\n",
    "    model_results['f1-score'] = 2*((tp/(tp+fp))*(tp/(tp+fn)))/((tp/(tp+fp))+(tp/(tp+fn)))\n",
    "\n",
    "    gs_results_cv = gs_results_cv.append(model_results, ignore_index=True)\n",
    "    pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "import seaborn as sns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
